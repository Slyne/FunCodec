# input related
input_size: 768
use_preprocessor: true
audio_max_duration: 30
codec_token_rate: 25

# network architecture
# encoder related
text_encoder: conformer
text_encoder_conf:
    output_size: 1024    # dimension of attention
    attention_heads: 16
    linear_units: 4096  # the number of units of position-wise feed forward
    num_blocks: 6      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: linear # encoder architecture type
    normalize_before: true
    rel_pos_type: latest
    pos_enc_layer_type: rel_pos
    selfattention_layer_type: rel_selfattn
    use_cnn_module: false

# decoder related
codec_encoder: conformer
codec_encoder_conf:
    output_size: 1024    # dimension of attention
    attention_heads: 16
    linear_units: 4096  # the number of units of position-wise feed forward
    num_blocks: 6      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: linear # encoder architecture type
    normalize_before: true
    rel_pos_type: latest
    pos_enc_layer_type: rel_pos
    selfattention_layer_type: rel_selfattn
    use_cnn_module: false

# model related
model: laura_gen_model
model_conf:
    codec_sampling_ratio: 0.5
    lsm_weight: 0.0
    length_normalized_loss: true
    predict_nq: 2
    codec_conf:
        num_quantizers: 32
        codebook_size: 1024
        codebook_dim: 128
    codec_lm_conf:
        name: transformer
        pos_enc: rel_pos
        selfattention_layer_type: rel_selfattn
        embed_unit: 128
        att_unit: 1024
        head: 16
        unit: 4096
        layer: 12
        dropout_rate: 0.1
        pe_type: uni
        bidirectional_inputs: true
        codec_groups: 1

# small dataloader related
batch_type: unsorted
batch_size: 32
num_workers: 8

# optimization related
optim: adam
optim_conf:
    lr: 0.001
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 10000

# training process
# num_iters_per_epoch: 10000
accum_grad: 1
grad_clip: 5
max_epoch: 50
val_scheduler_criterion:
    - valid
    - loss
best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 50
log_interval: 50
